{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Intro to Machine Learning (ML)\n",
    "\n",
    "## What is a Data Model?\n",
    "\n",
    "Data Models are formal structures to describe data. They are used to standardize the way data is described, stored, and accessed. Data models are used to represent the entities and their relationships within a database. They are used for data organization and to define the constraints on the collected data. Data models are used to define the data elements and their relationships. \n",
    "\n",
    "Remember that there are 3 types of different ML approaching:\n",
    "- The supervised ones, where we have a target variable to predict\n",
    "- The unsupervised ones, where we don't have a target variable to predict\n",
    "- The reinforcement ones, where we have a target variable to predict, but we don't have the data to train the model. The model will learn by itself by trying to reach a goal.\n",
    "\n",
    "## Training, Validation and Test Sets\n",
    "\n",
    "When we train a model, we need to split our data into 3 different sets:\n",
    "- The training set, which is used to train the model\n",
    "- The validation set, which is used to validate the model. The trained model is tested on the validation set to see how well it performs on unseen data.\n",
    "- The test set, which is used to test the model. The trained model is used on new data to see how well it performs on real-case.\n",
    "\n",
    "## A simple problem - Decision Tree\n",
    "\n",
    "In decision trees, we try to split the data into different groups based on the features. The goal is to find the best split to have the most homogeneous groups. The homogeneity is measured by the Gini index. The Gini index is a measure of the impurity of the data. The lower the Gini index, the more homogeneous the group is. The Gini index is defined as:\n",
    "\n",
    "$$Gini = 1 - \\sum_{i=1}^{n} p_i^2$$\n",
    "\n",
    "where $p_i$ is the probability of an item being classified to a particular class. The Gini index is maximum when the classes are equally distributed. It is minimum (zero) when all the items belong to a single class.\n",
    "\n",
    "## Ensemble models\n",
    "\n",
    "Ensemble models are models that combine several models to improve the performance of the model. The most famous ensemble model is the Random Forest. The Random Forest is a collection of decision trees. The Random Forest is a supervised learning algorithm. It can be used for both classification and regression. It is an ensemble of decision trees. It is a collection of decision trees. Also, there are models like Bagging, Boosting, Stacking, etc. \n",
    "\n",
    "For classification, it is also used a approach called Support Vector Classifier (SVC). It classifies data in 2 classes and starts to trace some lines to separate the data. The goal is to find the best line to separate the data. The best line is the one that has the maximum margin between the data points and the line. The data points that are closest to the line are called support vectors.\n",
    "\n",
    "## Accuracy, Precision, Recall, F1-Score\n",
    "\n",
    "Accuracy is the number of correct predictions divided by the total number of predictions. It is the most intuitive performance measure. But it is not a good measure when the data is imbalanced. For example, if we have 1000 data points and 950 of them belong to class A and 50 of them belong to class B, then the model can predict all the data points as class A and still get an accuracy of 95%. So, accuracy is not a good measure when the data is imbalanced.\n",
    "\n",
    "To handle this problem, we use precision and recall. Precision is the number of correct positive predictions divided by the total number of positive predictions. Recall is the number of correct positive predictions divided by the total number of positive data points. Precision and recall are inversely related. If we increase the precision, the recall decreases and vice versa. So, we need to find a balance between precision and recall. To find this balance, we use the F1-score. The F1-score is the harmonic mean of precision and recall. \n",
    "\n",
    "Moreover, it is also common to present the confusion matrix. The confusion matrix is a table that is often used to describe the performance of a classification model on a set of test data for which the true values are known. It allows the visualization of the performance of an algorithm. The confusion matrix shows the ways in which the classification model is confused when it makes predictions. It gives us insight not only into the errors being made by a classifier but more importantly the types of errors that are being made.\n",
    "\n",
    "In this topic, it is also presented the Receiver Operating Characteristic (ROC) curve. The ROC curve is a plot of the true positive rate (TPR) against the false positive rate (FPR). It shows the tradeoff between sensitivity and specificity. The area under the ROC curve is called AUC. The higher the AUC, the better the model is at predicting the target variable.\n",
    "\n",
    "# Unsupervised Learning Algorithms\n",
    "\n",
    "It is some algorithms to perform Dimensionality reduction algorithms, like Principal Component Analysis (PCA), Singular Value Decomposition (SVD), and t-Distributed Stochastic Neighbor Embedding (t-SNE). It is also presented the K-Means algorithm, which is a clustering algorithm. It is also presented the Hierarchical Clustering algorithm, which is another clustering algorithm. Moreover, a DL approach to make this type of classification is Autoencoders. Autoencoders are neural networks that try to reconstruct the input data. They are used for dimensionality reduction. They are also used for image denoising. This approaching are motivated by the fact that all the info is contained on a N-dim space, with N smaller than the original space.\n",
    "\n",
    "In dimensionality reduction algorithms, there is used quantitative features to see the similarity, such as the Pearson correlation coefficient. The Pearson correlation coefficient is a measure of the linear correlation between two variables X and Y. It has a value between +1 and −1, where 1 is total positive linear correlation, 0 is no linear correlation, and −1 is total negative linear correlation.\n",
    "\n",
    "On clustering, you need to take considerations based on your cluster definition, the metric adopted to measure distances and the feature you choose to consider to cluster data. K Means have been used in this topic, but it have some problems based on the randomly initialization of data. Thus, an implementation improved this problem is called k-means++\n",
    "\n",
    "# Dask\n",
    "\n",
    "Dask is a flexible library for parallel computing in Python. It is a library that allows us to scale our code from a single machine to a cluster. It can handle dataframes as a collection of Pandas Dataframe, and it can handle arrays as a collection of NumPy arrays. Dask is composed of two parts: Dynamic task scheduling optimized for computation. “Big Data” collections like parallel arrays, dataframes, and lists that extend common interfaces like NumPy, Pandas, or Python iterators to larger-than-memory or distributed environments. These parallel collections run on top of the dynamic task schedulers.\n",
    "\n",
    "# Distributed Parallel Running of DL models\n",
    "\n",
    "It can be done using Pytorch or using Accelerate. Accelerate is a library that allows us to scale our code from a single machine to a cluster. It is a library that allows us to scale our code from a single machine to a cluster. It can handle dataframes as a collection of Pandas Dataframe, and it can handle arrays as a collection of NumPy arrays. Accelerate is composed of two parts: Dynamic task scheduling optimized for computation. “Big Data” collections like parallel arrays, dataframes, and lists that extend common interfaces like NumPy, Pandas, or Python iterators to larger-than-memory or distributed environments. These parallel collections run on top of the dynamic task schedulers.\n",
    "\n",
    "The main advantage of using Accelerate is that it requires a minimal code change. We just need to change the imports and the data structures. We don't need to change the code. It is also possible to use Pytorch to do the same thing, but it requires more code changes. However, the main drawback relies on the fact that Accelerate not always works as well as Pytorch Distributed Data Parallel (DDP). It is also possible to use Pytorch DDP to do the same thing, but it requires more code changes. However, the main drawback relies on the fact that Pytorch DDP not always works as well as Accelerate. In Pytorch also it is needed to use torchrun to run the code in parallel.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cbff8209e4dc4485"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "b3ba6bed289b67ff"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
