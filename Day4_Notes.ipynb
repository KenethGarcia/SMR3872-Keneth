{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Overview of accelerated computer architectures\n",
    "\n",
    "Accelerators are a essential part of a HPC cluster. They are used to accelerate the performance of specific tasks. The most common accelerators are GPUs and FPGAs. In this section we will focus on GPUs.\n",
    "\n",
    "In the past, clusters was CPU based mainly, with critical code optimization and hardware-bounded. However, nowadays, there are heterogeneous architectures, with accelerators, and the code is optimized for the architecture. The code is also driven by AI and Machine Learning, it is data intensive and the code is framework-bounded.\n",
    "\n",
    "Actually, accelerators relies on GPUs. Mostly Nvidia bounded, the accelerators have a lot of cores connected to a memory. The memory is shared between the cores. The cores are grouped in Streaming Multiprocessors (SMs). The SMs are grouped in a GPU. The GPU is connected to the CPU via a PCI Express bus. The GPU is also connected to a memory via a memory bus.\n",
    "\n",
    "In this hybrid systems, there is always a cost of using GPU, due to the memory copying. Before running any program in GPU, it needs data to be transferred from the CPU memory to the GPU memory. After the program is executed, the data needs to be transferred back to the CPU memory. This is a very expensive operation, and it is important to minimize the number of data transfers. Leonardo's supercomputers consists on NVIDIA A100 GPUs system, based on Ampere architecture. The GPUs are connected to the CPU via a NVLink bus, which is faster than the PCI Express bus.\n",
    "\n",
    "# GPU Memory Hierarchy\n",
    "\n",
    "The GPU memory hierarchy is composed by different types of memory. The GPU memory hierarchy is composed by the following types of memory:\n",
    "\n",
    "- Global memory: it is the main memory of the GPU. It is the largest memory, but it is also the slowest. It is used to store the data that will be used by the GPU cores. It is also used to store the results of the GPU cores (L2 is part of the global memory).\n",
    "- Each thread has a local memory. It is the fastest memory, but it is also the smallest. It is used to store the data that will be used by the thread. It is also used to store the results of the thread.\n",
    "- Shared memory: it is a memory shared by all threads in a block. It is faster than the global memory, but it is slower than the local memory. It is used to store the data that will be used by all threads in a block. It is also used to store the results of all threads in a block.\n",
    "- L1, shared memory.\n",
    "\n",
    "Note: CPU architecture are optimized to minimize the latency of the memory access. GPU architecture are optimized to maximize the bandwidth of the memory access.\n",
    "\n",
    "# Memory Coalescing\n",
    "\n",
    "Global memory is organized in 32-bit or 128-bit words. The GPU cores can access the global memory in different ways. The GPU cores can access the global memory in a coalesced way or in a non-coalesced way. The coalesced way is the fastest way to access the global memory. The non-coalesced way is the slowest way to access the global memory. In coalescing processes, the GPU cores access the global memory in a constant way. In non-coalescing processes, the GPU cores access the global memory in a sequential way.\n",
    "\n",
    "# SIMD and SIMT\n",
    "\n",
    "SIMD stands for Single Instruction Multiple Data. It is a type of parallelism where the same instruction is executed by multiple cores. The cores execute the instruction on different data. The cores are independent, and they do not communicat In this approach, data must be loaded and stored in contiguous buffers.\n",
    "\n",
    "On the other hand, SIMT stands for Single Instruction Multiple Thread. It is a type of parallelism where the same instruction is executed by multiple threads. The threads execute the instruction on different data. The threads are independent and they do not communicate. In this approach, data must not be loaded and stored in contiguous buffers, then contiguous data is not required.\n",
    "\n",
    "# Programming Models for GPUs\n",
    "\n",
    "Actually, there are some programming models for GPUs. The most common are:\n",
    "\n",
    "- CUDA: it is a programming model developed by NVIDIA. It is a proprietary programming model. It is based on C/C++ language. It is the most common programming model for GPUs.\n",
    "- OpenMP: it is a programming model developed by OpenMP Architecture Review Board. It is an open source programming model. It is based on C/C++/Fortran language. It is the most common programming model for CPUs.\n",
    "- OpenACC: it is a programming model developed by OpenACC Organization. It is an open source programming model. It is based on C/C++/Fortran language.\n",
    "- Kokkos: it is a programming model developed by Sandia National Laboratories. It is an open source programming model. It is based on C/C++/Fortran language.\n",
    "- SYCL: it is a programming model developed by Khronos Group. It is an open source programming model. It is based on C++ language. It relies on OpenCL, CUDA, ROCm, SPIR-V, etc.\n",
    "\n",
    "Each of one has a level of control and portability tradeoff, so it is important to note the differences between them."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2322d5194bddca18"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# OpenACC - Directive Based Programming for Hardware Accelerators\n",
    "\n",
    "OpenACC is a programming model for hardware accelerators. It is based on directives. It is a high-level programming model. It is based on C/C++/Fortran language. It is an open source programming model. It is supported by NVIDIA, AMD, IBM, ARM, etc. It is supported by PGI, GCC, Clang, etc.\n",
    "\n",
    "Remember that GPUs depends on CPU to run. The CPU is the host and the GPU is the device. At least for now, GPUs can't be used without a CPU. The CPU is responsible for the data transfers between the host and the device. The CPU is also responsible for the data transfers between the device and the host.\n",
    "\n",
    "It is important to note that the slowest part of the computing part of GPUs is the time spent on data transfers. The data transfers are the most expensive part of the computing part of GPUs. It is important to minimize the number of data transfers between the host and the device. It is also important to minimize the number of data transfers between the device and the host.\n",
    "\n",
    "The main structure of a code using OpenACC is:\n",
    "\n",
    "```c\n",
    "int main() {\n",
    "    // Host code\n",
    "    // ...\n",
    "    #pragma acc data kernels // (i.e. copyin(A[0:N], B[0:N]) copyout(C[0:N]))\n",
    "    {\n",
    "        // Device code\n",
    "        // ...\n",
    "        #pragma acc parallel loop\n",
    "        for (int i = 0; i < N; i++) {\n",
    "            // Device code\n",
    "            // ...\n",
    "        }\n",
    "        // Device code\n",
    "        // ...\n",
    "    }\n",
    "    // Host code\n",
    "    // ...\n",
    "}\n",
    "```\n",
    "where the host code is executed by the host and the device code is executed by the device. The host code is executed by the CPU and the device code is executed by the GPU."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a472f97d8e139b7e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "bd3d44c4c275b3c1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
